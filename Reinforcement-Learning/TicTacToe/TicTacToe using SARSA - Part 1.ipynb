{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TicTacToe using SARSA Part 1 - Exact States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will try to create a TicTacToe Environment and Agent, while saving all states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using the SARSA algorithm, which enables the agent to learn after every step using \"bootstrapping\", instead of waiting for the end of the game (\"episode\"). You can read more about it in the [Sutton & Barto's book (2nd edition)](https://d3c33hcgiwev3.cloudfront.net/Ph9QFZnEEemRfw7JJ0OZYA_808e8e7d9a544e1eb31ad11069d45dc4_RLbook2018.pdf?Expires=1578700800&Signature=kknv~Fe2hgmHae7aID4u9P9BUwvcIQ2F5qaNopIiaOpjUeiqESW6W4xhnji1Yyf1dEgNg5NvaKCqAOtHPX65N4LFHM3cU-Zj3WQFRl1S~NM79uQSWijIvnCNAIvPVSLct6i5u7Ruc-IkWYDGoPFtyHUWq8iFH1WJBuOZTaw~QzQ_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A), in section 6.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using a few base classes that I lent from the Coursera courses on Reinforcement Learning - mainly the RLGlue, which I modified. This is used as a glue between an agent and an environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BaseAgent import BaseAgent\n",
    "from BaseEnvironment import BaseEnvironment\n",
    "from RLGlue import RLGlue\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload modules in case I change them\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment can be modeled as an array of 9 values, reshaped to a 3x3 matrix. 0 means there's no x or O in that square.\n",
    "The actual player values will be given from the outside (spoiler alert, they will be -1 and +1). I also return a mask that will be used by the agent in the policy action decision to determine which available options it has. The mask has 0 for taken squares, and 1 for free squares.\n",
    "\n",
    "I give a +10 reward for winning, 0 reward for tie. In order to encourage fast games, I give a reward of -1 for every step that doesn't result in winning. I also give a negative reward of -10 for the agent that lost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeEvnironment(BaseEnvironment):\n",
    "    def env_init(self, env_info={}):\n",
    "        pass\n",
    "    \n",
    "    def env_start(self):\n",
    "        self.terminal = False\n",
    "        self.board = np.zeros((3, 3))\n",
    "        self.reward_obs_term = (0, self.board, False)\n",
    "        return self.board.copy().reshape(9), self.get_mask()\n",
    "    \n",
    "    def env_step(self, agent_num, index):\n",
    "        if self.terminal:\n",
    "            print(\"Environment in terminal state, please restart.\")\n",
    "        \n",
    "        row, col = self.transform_index(index)\n",
    "        self.board[row, col] = agent_num\n",
    "        \n",
    "        if self.check_won(agent_num):\n",
    "            reward = 10\n",
    "            self.terminal = True\n",
    "        elif self.check_tie():\n",
    "            reward = 0\n",
    "            self.terminal = True\n",
    "        else:\n",
    "            reward = -1\n",
    "            \n",
    "        self.reward_obs_term_mask = (reward, self.board.copy().reshape(9), self.terminal, self.get_mask())\n",
    "        return self.reward_obs_term_mask\n",
    "    \n",
    "    def check_tie(self):\n",
    "        return (self.board == 0).sum() == 0\n",
    "    \n",
    "    def check_won(self, agent_num):\n",
    "        for row in self.board:\n",
    "            if np.array_equal(row, agent_num * np.ones((3,))):\n",
    "                return True\n",
    "        for col in self.board.T:\n",
    "            if np.array_equal(col, agent_num * np.ones((3,))):\n",
    "                return True\n",
    "        diag = np.diag(self.board)\n",
    "        if np.array_equal(diag, agent_num * np.ones((3,))):\n",
    "            return True\n",
    "        diag = np.diag(np.fliplr(self.board))\n",
    "        if np.array_equal(diag, agent_num * np.ones((3,))):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def env_cleanup(self):\n",
    "        pass\n",
    "    \n",
    "    def env_message(self, message):\n",
    "        if message == 0:  # return available indices mask\n",
    "            return self.get_mask()\n",
    "            \n",
    "    def get_mask(self):\n",
    "        rows, cols = np.where(self.board == 0)\n",
    "        indices = rows * 3 + cols\n",
    "        mask = np.zeros((9,))\n",
    "        mask[indices] = 1\n",
    "        return mask.astype(int)\n",
    "    \n",
    "    def transform_index(self, index):\n",
    "        return index // 3, index % 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent will keep track of all seen states in his Q(s,a) table (called here states). Theoratically, there are 9 squares on the board, and each can contain 3 values (empty, x, or O) , so there can be 3^9 =~ 20k different states. But most of these states are probably not feasible for a game. So it's much better if we simply add every new encountered state to a dictionary.\n",
    "\n",
    "The policy will choose greedily (online) the best possible action from all available ones. It will break the tie between equally good options arbitrarily. If the state hasn't been encountered yet, all available spaces get same probability to be chosen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeAgent(BaseAgent):\n",
    "    def agent_init(self, agent_init_info):    \n",
    "        self.learning_step = agent_init_info[\"learning_step\"]\n",
    "        self.num_actions = agent_init_info[\"num_actions\"]\n",
    "        self.rand_generator = np.random.RandomState(agent_init_info[\"seed\"])\n",
    "        self.states = {}\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "\n",
    "    def policy(self, q, mask):\n",
    "        # greedy policy, breaks ties randomly\n",
    "        pos = np.exp(q - np.max(q)) * mask\n",
    "        return self.argmax(pos)\n",
    "\n",
    "    def argmax(self, array):\n",
    "        m = np.max(array)\n",
    "        ind = np.where(array == m)[0]\n",
    "        return self.rand_generator.choice(ind)\n",
    "    \n",
    "    def get_q(self, state, mask):\n",
    "        if str(state) in self.states:\n",
    "            q = self.states[str(state)]\n",
    "        else:\n",
    "            q = np.zeros_like(state)\n",
    "            q[mask == 1] = 1\n",
    "        return q\n",
    "    \n",
    "    def agent_start(self, state, mask):\n",
    "        self.last_state = state\n",
    "        q = self.get_q(state, mask)\n",
    "        self.states[str(self.last_state)] = q\n",
    "        self.last_action = self.policy(q, mask)\n",
    "        return self.last_action        \n",
    "\n",
    "    def agent_step(self, reward, state, mask):\n",
    "        # SARSA\n",
    "        q = self.get_q(state, mask)\n",
    "        action = self.policy(q, mask)\n",
    "        q_s_ = q[action]\n",
    "        q_s = self.states[str(self.last_state)][self.last_action]\n",
    "        self.states[str(self.last_state)][self.last_action] += self.learning_step * (reward + q_s_ - q_s)\n",
    "\n",
    "        self.last_state = state\n",
    "        self.states[str(self.last_state)] = q\n",
    "        self.last_action = action\n",
    "        return action\n",
    "    \n",
    "    def agent_end(self, reward):\n",
    "        # SARSA\n",
    "        q_s = self.states[str(self.last_state)][self.last_action]\n",
    "        self.states[str(self.last_state)][self.last_action] += self.learning_step * (reward - q_s)\n",
    "        \n",
    "    def agent_message(self, message):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment will use the environment coupled with 2 agents, one which will change states from 0 to -1, and the other from 0 to +1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(runs):\n",
    "    rlglue = RLGlue(TicTacToeEvnironment, TicTacToeAgent, TicTacToeAgent)\n",
    "    rewards = np.zeros((runs, 2))\n",
    "    starts = np.zeros((runs, 1))\n",
    "    final_states = np.zeros((runs, 9))\n",
    "    agent1_info = {\"learning_step\": 1, \"num_actions\": 9, \"seed\": 12}\n",
    "    agent2_info = {\"learning_step\": 1, \"num_actions\": 9, \"seed\": 17}\n",
    "    env_info = {}\n",
    "    rlglue.rl_init(agent1_info, agent2_info, env_info)\n",
    "\n",
    "    for i in tqdm(range(runs)):\n",
    "        rlglue.rl_episode(10)\n",
    "        rewards[i, :], starts[i, :], final_states[i, :] =  rlglue.rl_return()\n",
    "    return rlglue, rewards, starts, final_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:47<00:00, 210.87it/s]\n"
     ]
    }
   ],
   "source": [
    "rlg, rew, starts, states = experiment(30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the last 100 games rewards were for both players "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(rew[-100:,])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agents reach a deadlock of tie against each other after around 30k episodes! This actually improves over time, after 10k agents tie, but not always, and it improves for 20k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 9)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agent1 wins, agent2 wins\n",
    "(rew[:,0] > 0).sum() , (rew[:,1] > 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both agents win only a miniscule fraction of games... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's play against an AI agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TicTacToeEvnironment()\n",
    "env.env_init()\n",
    "ret = env.env_start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai = rlg.agents[-1][\"agent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = ai.agent_start(ret[0], ret[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = env.env_step(-1, action) # ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = env.env_step(1, 4) # me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What returns: reward, state, terminal, mask; \n",
    "The agent_step needs reward, state and mask - I put the reward manually just for no reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = ai.agent_step(-1, ret[1], ret[3]) \n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = env.env_step(-1, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = env.env_step(1, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = ai.agent_step(-1, ret[1], ret[3])\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = env.env_step(-1, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = env.env_step(1, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = ai.agent_step(-1, ret[1], ret[3])\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,\n",
       " array([-1., -1., -1.,  0.,  1., -1.,  0.,  1.,  1.]),\n",
       " True,\n",
       " array([0, 0, 0, 1, 0, 0, 1, 0, 0]))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env_step(-1, action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ai wins ! (reward of 10 for it's move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets save this agent\n",
    "fw = open('agent-exact-states', 'wb')\n",
    "pickle.dump(ai.states, fw)\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4484"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many unique states are there?\n",
    "len(ai.states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that even though theoratically there could be a maximum 20k states, in reality the agents only saw less than 5k states. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was good - we now have an agent that can probably beat us in Tic-Tac-Toe and it was trained pretty fast. The only downside is that we had to save all the states that the evnironment could produce. For Tic-Tac-Toe this isn't bad, but for more complex scenarios it can become unfeasable. So can we do better?\n",
    "\n",
    "Let's see in part 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
